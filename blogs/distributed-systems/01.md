# Distributed Systems: The Backbone of Your Digital World

Think about your everyday life. Almost everything you do is powered by a network of systems.

Want to listen to your favorite song? You open Spotify. Craving a movie or a quick video? You’re on YouTube. Need something delivered? Amazon’s got you covered. Planning dinner? You’re scrolling through Zomato or Blinkit. Staying in touch with friends? WhatsApp video calls or other chat apps are just a tap away.

The internet has revolutionized how we live, but here’s something to ponder: Would the internet still feel so indispensable if the services we rely on weren’t always available, whenever we needed them?

The magic lies in their reliability—these systems are always on, ready to serve you, no matter where you are. It’s seamless. It’s beautiful. But have you ever wondered how they pull this off?

Welcome to the fascinating world of distributed systems—the silent engines behind your favorite apps. In this series, we’ll dive deep into how these systems work and what makes them tick.

## The basics
Behind all these services, we have some computers performing their jobs. Each computer has four workhorses to employ - CPU, Memory, Network and Storage.

For a small system where we deal with a small set of data or small number of user requests. All these components can be part of a single Computer System.

## Hardware Expansion
Every sub-systems in the computer has some limit. So, a computer system's service capacity is dependent on the limits of these sub-systems. On the other hand, service demands from your application increases over time.

We can mitigate this problem by increasing the capacity of underlying hardwares. We can employ more powerful CPU, expanded RAM, higher bandwidth. This process of expanding capacity by employing powerful hardwares is known as Vertical Scaling.

## Multi-Tier architecture
Now Vertical Scaling can go up to some extent and mitigate the issue. However, this system will also reach a limit. The simplest work around is to break down the application based on the task they perform. For example, one can think of a system having three distinguised components - 
1. Presentation layer - this layer interacts with the user for example a web app whose primary responsibility is to handle user interaction. For example, your banking website.
2. The Logic layer - this layer is responsible is for all the business logic handling. For example, a banking server can have a layer which handles all the transactions.
3. The Data Layer - this layer is responsible for persisting and querying of data. For example, the account database in a banking system.

## Parallel Processing
With Vertical Scaling and Multi-Tier Architecture, we can temporarily serve more requests. But at certain point, physical constraints kick in. We cannot infinitely grow a system with these techniques.

What happens when we reach that limit?

Well, requests start to pile up and system start performing poorly. As a result, throughput of the system decreases and system responds slowly. This is problematic from user perspective. Imagine, you are transferring funds but due to limiting factors in the system you keep on waiting. This results in anxeity and you get frustrated.

To avoid this problem, we employ more machines to help or in simple terms, we distribute the work load in different machines. This is called Horizontal Scaling.

Thus, Distributed Systems is an architecture where we employ multiple interconnected nodes to achieve parallel processing of requests and evenly distribute work load in multiple machines.

## The Problem with Distributed Systems
Now, we can process more requests and theoritically scale infinitely. But, this does not come for free. When we deal with small number of systems, failure rates are ridiculuously low. But when we deal with high number of systems, we see how the ignorable possibility of hardware failures also scale up.

Let's take an example. If we deal with 1 system with failure probablity of 1 failure every 1000 days. So, the probablity of that machine failing is 0.001 per day, which is negligible. However, if we employ 1000 such systems, the failure rate becomes 1 failure a day. Now, this is concerning. And in fact, a reality where thousands of systems are interconnected. Hardware failures are more common than you can think.


